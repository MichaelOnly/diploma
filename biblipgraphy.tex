\begin{thebibliography}{00}
   \addcontentsline{toc}{chapter}{СПИСОК ЛИТЕРАТУРЫ}
   \bibitem{bayes}
     \textit{Rish I. et al.} An empirical study of the naive Bayes classifier // IJCAI 2001 workshop on empirical methods in artificial intelligence. – 2001. – Т. 3. – №. 22. – С. 41-46.
   \bibitem{svm}
     \textit{Cortes C., Vapnik V.} Support-vector networks // Machine learning. – 1995. – Т. 20. – С. 273-297.
   \bibitem{gelu}
     \textit{Hendrycks D., Gimpel K.} Gaussian error linear units (gelus) // arXiv preprint arXiv:1606.08415. – 2016 (дата обр. 13.06.2023).
   \bibitem{backprop}
     \textit{Hecht-Nielsen R.} Theory of the backpropagation neural network // Neural networks for perception. – Academic Press, 1992. – С. 65-93.
   \bibitem{transformers}
     \textit{Vaswani A. et al.} Attention is all you need // Advances in neural information processing systems. – 2017. – Т. 30.
   \bibitem{bert}
     \textit{Devlin J. et al.} Bert: Pre-training of deep bidirectional transformers for language understanding // arXiv preprint arXiv:1810.04805. – 2018 (дата обр. 13.06.2023).
   \bibitem{roberta}
     \textit{Liu Y. et al.} Roberta: A robustly optimized bert pretraining approach // arXiv preprint arXiv:1907.11692. – 2019 (дата обр. 13.06.2023).
   \bibitem{distilbert}
     \textit{Sanh V. et al.} DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter // arXiv preprint arXiv:1910.01108. – 2019 (дата обр. 13.06.2023).
   \bibitem{albert}
     \textit{Lan Z. et al.} Albert: A lite bert for self-supervised learning of language representations // arXiv preprint arXiv:1909.11942. – 2019 (дата обр. 13.06.2023).
   \bibitem{electra}
     \textit{Clark K. et al.} Electra: Pre-training text encoders as discriminators rather than generators // arXiv preprint arXiv:2003.10555. – 2020 (дата обр. 13.06.2023).
   \bibitem{opt}
     \textit{Zhang S. et al.} Opt: Open pre-trained transformer language models // arXiv preprint arXiv:2205.01068. – 2022 (дата обр. 13.06.2023).
   \bibitem{t5}
     \textit{Raffel C. et al.} Exploring the limits of transfer learning with a unified text-to-text transformer // The Journal of Machine Learning Research. – 2020. – Т. 21. – №. 1. – С. 5485-5551.
   \bibitem{open-assistant}
     \textit{Koepf A. et al.} OpenAssistant Conversations--Democratizing Large Language Model Alignment // arXiv preprint arXiv:2304.07327. – 2023 (дата обр. 13.06.2023).
   \bibitem{paper}
     \textit{Бурлаков В. С., Ишутин М. А., Пятанин А. А.} Диалоговая система для разработчиков видеоигр // Наука. Технологии. Инновации : сб. науч. тр. : в 11 ч., Новосибирск, 05-08 дек. 2022 г. Ч. 2 -- Изд-во НГТУ, 2022. -- C. 101-104.
   \bibitem{py}
     Документация Python [Электронный ресурс]. URL: https://docs.python.org/3/ (дата обр. 13.06.2023).
   \bibitem{pd}
     Документация Pandas [Электронный ресурс]. URL: https://pandas.pydata.org/docs/ (дата обр. 13.06.2023).
   \bibitem{transformer}
     Документация Transformers [Электронный ресурс]. URL: https://huggingface.co/docs/transformers/index (дата обр. 13.06.2023).
   \bibitem{datasets}
     Документация Datasets [Электронный ресурс]. URL: https://huggingface.co/docs/datasets/index (дата обр. 13.06.2023).
   \bibitem{evaluate}
     Документация Evaluate [Электронный ресурс]. URL: https://huggingface.co/docs/evaluate/index (дата обр. 13.06.2023).
   \bibitem{wandb}
     Документация Weights\&Biases [Электронный ресурс]. URL: https://docs.wandb.ai/ (дата обр. 13.06.2023).
   \bibitem{tqdm}
     Документация tqdm [Электронный ресурс]. URL: https://tqdm.github.io/ (дата обр. 13.06.2023).
   \bibitem{numpy}
     Документация NumPy [Электронный ресурс]. URL: https://numpy.org/doc/ (дата обр. 13.06.2023).
   \bibitem{matplotlib}
     Документация Matplotlib [Электронный ресурс]. URL: https://matplotlib.org/stable/index.html (дата обр. 13.06.2023).
   \end{thebibliography}